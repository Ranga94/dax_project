{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fundamental and Stock Price Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This code analyse the historical data and near-real time data to find general trends in equities. The types of analysis shown here are: \n",
    "\n",
    "### -Cumulative returns for long, medium and short term\n",
    "### -Estimating the average quarterly growth of closing price\n",
    "### -Volatility analysis (Average True Range, standard deviation of closing price)\n",
    "### -Financial performance analysis (Return on Capital + Sales Revenue)\n",
    "### -Categorising equities by industry\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting historial share price data from MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pymongo\n",
    "from re import sub\n",
    "from decimal import Decimal\n",
    "from pymongo import MongoClient\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "import scipy\n",
    "from scipy import stats\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from odo import odo\n",
    "from decimal import Decimal\n",
    "import operator\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "\n",
    "#client = MongoClient('mongodb://igenie:igenie@ds019654.mlab.com:19654/dax')\n",
    "client_old = MongoClient('mongodb://admin:admin@ds019654.mlab.com:19654/dax')\n",
    "client_new = MongoClient('mongodb://igenie_readwrite:igenie@35.189.101.142:27017/dax_gcp')\n",
    "#get dax database\n",
    "db_old = client_old.dax\n",
    "db = client_new.dax_gcp\n",
    "#collection = db['historical']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "collection = db['historical']\n",
    "his = collection.find({\"constituent\":'Allianz'})\n",
    "his = pd.DataFrame(list(his))\n",
    "#his_rm21 = his['closing_price'].rolling(window=21,center=False).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_constituents = ['Allianz', 'adidas', 'BASF', 'Bayer', 'Beiersdorf','BMW', 'Commerzbank', 'Continental', 'Daimler','Deutsche Bank', 'Deutsche Börse', 'Deutsche Post','Deutsche Telekom', 'EON', 'Fresenius', 'HeidelbergCement', 'Infineon','Linde','Lufthansa', 'Merck', 'RWE', 'SAP', 'Siemens', 'thyssenkrupp','Vonovia','Fresenius Medical Care','Münchener Rückversicherungs-Gesellschaft','ProSiebenSat1 Media','Volkswagen (VW) vz']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deducing Profitability "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cumulative Return analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This section calculates the cumulative return of each equity, as if they were invested 6 months, 1 year and 5 years ago. It also returns rankings of top 5 with the best cumulative return and the worst cumulative return for these periods of investments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Cumulative Return (6 months, 1 year, 5 years)\n",
    "##Table 2 stores the annual mean price and the annual mean growth\n",
    "def cumulative_returns_collection():\n",
    "    collection = db['historical']\n",
    "    n=0\n",
    "    cumulative_table = pd.DataFrame()\n",
    "    all_constituents = ['Allianz', 'adidas', 'BASF', 'Bayer', 'Beiersdorf','BMW', 'Commerzbank', 'Continental', 'Daimler','Deutsche Bank', 'Deutsche Börse', 'Deutsche Post','Deutsche Telekom', 'EON', 'Fresenius', 'HeidelbergCement', 'Infineon','Linde','Lufthansa', 'Merck', 'RWE', 'SAP', 'Siemens', 'thyssenkrupp','Vonovia','Fresenius Medical Care','Münchener Rückversicherungs-Gesellschaft','ProSiebenSat1 Media','Volkswagen (VW) vz']\n",
    "    #all_constituents = ['Allianz', 'adidas', 'BASF', 'Bayer']\n",
    "    for constituent in all_constituents:\n",
    "        \n",
    "        n=n+1\n",
    "        his = collection.find({\"constituent\":constituent})\n",
    "        his = pd.DataFrame(list(his))\n",
    "        his = his.iloc[::-1] #So the most recent data is on the last row, for the convinience of analysis.\n",
    "        \n",
    "        \n",
    "        ##Compute the 21-days moving average of the closing price. \n",
    "        his_rm21=his['closing_price'].rolling(window=21,center=False).mean()\n",
    "        his_6months = his_rm21.iloc[-126:].reset_index(drop=True)\n",
    "        #print float(his_6months.iloc[-1])\n",
    "        his_1year = his_rm21.iloc[-252:].reset_index(drop=True)\n",
    "        #print float(his_1year.iloc[-1])\n",
    "        his_3years = his_rm21.iloc[-756:].reset_index(drop=True)\n",
    "        \n",
    "    ##Calculate the cumulative returns\n",
    "        return_6months =  (float(his_6months.iloc[-1])/float(his_6months.iloc[0]))-1.0\n",
    "        return_1year =  (float(his_1year.iloc[-1])/float(his_1year.iloc[0]))-1.0\n",
    "        return_3years =  (float(his_3years.iloc[-1])/float(his_3years.iloc[0]))-1.0\n",
    "        \n",
    "    ##Develop scoring system for cumulative returns\n",
    "        if (return_6months >0)&(return_1year>0)&(return_3years>0):\n",
    "            score = 4\n",
    "        elif ((return_6months >0)&(return_3years>0)) or (return_6months >0)&(return_1year>0):\n",
    "            score = 3\n",
    "        elif (return_3years>0)&(return_1year >0):\n",
    "            score = 2\n",
    "        elif (return_3years>0) or (return_1year >0) or (return_6months >0):\n",
    "            score = 1\n",
    "        else: \n",
    "            score = 0\n",
    "        \n",
    "\n",
    "    #append the values \n",
    "        cumulative_table = cumulative_table.append(pd.DataFrame({'Constituent': constituent, '6 months return': return_6months, '1 year return':return_1year,'3 years return': return_3years,'Cumulative return consistency score':score,'Table':'cumulative return analysis','Date':str(datetime.date.today())}, index=[0]), ignore_index=True)\n",
    "    \n",
    "    columnsTitles=['Constituent','6 months return','1 year return','3 years return','Cumulative return consistency score','Table','Date']\n",
    "    cumulative_table =cumulative_table .reindex(columns=columnsTitles)\n",
    "    #cumulative_table.to_csv('cumulative_table %s.csv'%datetime.date.today(), encoding = 'utf-8', index = False)\n",
    "\n",
    "    return cumulative_table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculating the means and standard deviations for 1-year and 3-year returns\n",
    "# Used to identify exceptionally performing stocks in terms of cumulative returns\n",
    "def CR_stats(cumulative_table): \n",
    "    mean_cr_1year = cumulative_table['1 year return'].mean()\n",
    "    mean_cr_3years = cumulative_table['3 years return'].mean()\n",
    "    std_cr_1year = cumulative_table['1 year return'].std()\n",
    "    std_cr_3years = cumulative_table['3 years return'].std()\n",
    "    return mean_cr_1year,mean_cr_3years,std_cr_1year,std_cr_3years"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigating the trend in stock price (quarterly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The average stock price for each quarter is calculated from 2010-01-01 (filtering the effect of recession in 2009). For each stock, a linear regression is fitted for the mean quarter prices for different three time durations.\n",
    "1. from 2010-01-01\n",
    "2. the last three years\n",
    "3. the last 12 months"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The gradient of the linear regression model estimates the rate of change in average price per quarter (€ /quarter or 3 months). By analysing the gradients derived from the three time durations above, we can see how the rate of change vary over time. If the gradient increases from period 1 to period 3, then a trend of accelerated growth in stock price is indicated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def quarter_mean_analysis(his):\n",
    "    #Analyse the cumulative return of the stock price after the recession in 2009. Quarterly. \n",
    "    his_2010 = his[['closing_price','date']].loc[his['date']>=datetime.datetime(2010,01,01)]\n",
    "    ##Calulate the mean stock price for every quarter\n",
    "    n=his_2010.shape[0]\n",
    "    num_quarters = int(n/63.0)\n",
    "    quarter_mean = np.zeros(num_quarters)\n",
    "    \n",
    "    for i in range(num_quarters): \n",
    "        if i<=num_quarters-1:\n",
    "            quarter_mean[i]=float(his_2010['closing_price'].iloc[63*i:63*(i+1)].mean())\n",
    "        else: \n",
    "            quarter_mean[i]=float(his_2010['closing_price'].iloc[63*i:].mean())\n",
    "            \n",
    "    z = np.polyfit(range(num_quarters),quarter_mean,1)\n",
    "    z_3yrs =np.polyfit(range(12), quarter_mean[-12:],1)\n",
    "    z_1yr = np.polyfit(range(4), quarter_mean[-4:],1)\n",
    "    return z[0],z[1],z_3yrs[0],z_3yrs[1],z_1yr[0],z_1yr[1],quarter_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def quarter_mean_collection(): \n",
    "    n=0\n",
    "    collection1 = db['historical']\n",
    "    quarter_mean_table = pd.DataFrame()\n",
    "    all_constituents = ['Allianz', 'adidas', 'BASF', 'Bayer', 'Beiersdorf','BMW', 'Commerzbank', 'Continental', 'Daimler','Deutsche Bank', 'Deutsche Börse', 'Deutsche Post','Deutsche Telekom', 'EON', 'Fresenius', 'HeidelbergCement', 'Infineon','Linde','Lufthansa', 'Merck', 'RWE', 'SAP', 'Siemens', 'thyssenkrupp','Vonovia','Fresenius Medical Care','Münchener Rückversicherungs-Gesellschaft','ProSiebenSat1 Media','Volkswagen (VW) vz']\n",
    "    #all_constituents = ['Allianz']\n",
    "    for constituent in all_constituents:\n",
    "        his=collection1.find({\"constituent\":constituent})\n",
    "        his = pd.DataFrame(list(his))\n",
    "        his = his.iloc[::-1]\n",
    "        \n",
    "        #assume linear model: y=ax+b\n",
    "        a,b,a_3yrs,b_3yrs,a_1yr,b_1yr,quarter_mean=quarter_mean_analysis(his)\n",
    "        \n",
    "        if (a_1yr>0)&(a>0)&(a_3yrs>0):\n",
    "            score =4\n",
    "        elif (a_1yr>0)&(a>0):\n",
    "            score = 3\n",
    "        elif ((a_1yr>0)&(a_3yrs>0)) or ((a>0)&(a_3yrs>0)):\n",
    "            score = 2\n",
    "        elif (a_1yr>0) or (a>0):\n",
    "            score = 1\n",
    "        else:\n",
    "            score = 0\n",
    "        \n",
    "        quarter_mean_table = quarter_mean_table.append(pd.DataFrame({'Constituent': constituent, 'Current Quarter mean price':round(quarter_mean[-1],2),'Rate of change in price from 2010/quarter': round(a,2), 'Rate of change in price in the last 3 years/quarter':round(a_3yrs,2),'Rate of change in price in the last 365 days/quarter': round(a_1yr,2),'Quarterly growth consistency score':score,'Table':'quarterly growth analysis','Date':str(datetime.date.today())}, index=[0]), ignore_index=True)\n",
    "    columnsTitles = ['Constituent','Current Quarter mean price','Rate of change in price from 2010/quarter', 'Rate of change in price in the last 3 years/quarter','Rate of change in price in the last 365 days/quarter','Quarterly growth consistency score','Table','Date']\n",
    "    quarter_mean_table =quarter_mean_table.reindex(columns=columnsTitles)\n",
    "    #quarter_mean_table.to_csv('quarter_mean_table.csv', encoding = 'utf-8', index = False)\n",
    "    return quarter_mean_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculating the means and standard deviations for 1-year and 3-year returns\n",
    "# Used to identify exceptionally performing stocks in terms of quarter mean rate of growth.\n",
    "def QM_stats(quarter_mean_table): \n",
    "    mean_qm_1year = quarter_mean_table['Rate of change in price in the last 365 days/quarter'].mean()\n",
    "    mean_qm_3years = quarter_mean_table['Rate of change in price in the last 3 years/quarter'].mean()\n",
    "    std_qm_1year = quarter_mean_table['Rate of change in price in the last 365 days/quarter'].std()\n",
    "    std_qm_3years = quarter_mean_table['Rate of change in price in the last 3 years/quarter'].std()\n",
    "    return mean_qm_1year,mean_qm_3years,std_qm_1year,std_qm_3years"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deducing Volatility\n",
    "\n",
    "### -Standard Deviation\n",
    "### -ATR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Deviation of closing price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Calculate standard deviation and Bollinger Bands, then plot. \n",
    "def Bollinger(his):\n",
    "    standard_dev = his['closing_price'].rolling(window=21,center=False).std()\n",
    "    upper = his['closing_price'].rolling(window=21,center=False).mean() + standard_dev*2.0\n",
    "    lower = his['closing_price'].rolling(window=21,center=False).mean() - standard_dev*2.0\n",
    "    ##Sport extreme values,record the number of times they happen.\n",
    "    above = (his['closing_price']>=upper)\n",
    "    below = (his['closing_price']<=lower)\n",
    "    above_dates = his.loc[above, 'date']\n",
    "    below_dates = his.loc[below,'date']\n",
    "    n_above = above_dates.shape[0]\n",
    "    n_below = below_dates.shape[0]\n",
    "    return n_above,n_below,standard_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Calculate the mean Standard Deviation quarterly in the last 18 months\n",
    "def standard_dev_collection():\n",
    "    n=0\n",
    "    collection = db['historical']\n",
    "    standard_dev_table = pd.DataFrame()\n",
    "    all_constituents = ['Allianz', 'adidas', 'BASF', 'Bayer', 'Beiersdorf','BMW', 'Commerzbank', 'Continental', 'Daimler','Deutsche Bank', 'Deutsche Börse', 'Deutsche Post','Deutsche Telekom', 'EON', 'Fresenius', 'HeidelbergCement', 'Infineon','Linde','Lufthansa', 'Merck', 'RWE', 'SAP', 'Siemens', 'thyssenkrupp','Vonovia','Fresenius Medical Care','Münchener Rückversicherungs-Gesellschaft','ProSiebenSat1 Media','Volkswagen (VW) vz']\n",
    "    for constituent in all_constituents:\n",
    "        n=n+1\n",
    "        his = collection.find({\"constituent\":constituent})\n",
    "        his = pd.DataFrame(list(his))\n",
    "        his = his.iloc[::-1]\n",
    "        \n",
    "        #print constituent\n",
    "        above,below,standard_dev=Bollinger(his)\n",
    "        std_3 = standard_dev[-63:].mean()\n",
    "        std_3yrs = standard_dev[-756:].mean()\n",
    "        std_1yr = standard_dev[-252:].mean()\n",
    "        ##Set a parameter to measure the stability of the stocks for the last 18 months\n",
    "        #std_mean = (std_3+std_3_to_6+std_6_to_9+std_9_to_12+std_12_to_15+std_15_to_18)/6.0\n",
    "        #standard_dev_table = standard_dev_table.append(pd.DataFrame({'Constituent': constituent, 'Last 3 months': round(std_3,3), 'Last 3-6 months':round(std_3_to_6,3),'Last 6-9 months': round(std_6_to_9,3),'Last 9-12 months':round(std_9_to_12,3), 'Last 12-15 months':round(std_12_to_15,3),'Last 15-18 months':round(std_15_to_18,3),'Mean std dev(quarterly)':round(std_mean,3)}, index=[0]), ignore_index=True)\n",
    "        standard_dev_table = standard_dev_table.append(pd.DataFrame({'Constituent': constituent,'Last 12 months':round(std_1yr,2),'Last 3 years':round(std_3yrs,2),'Table': 'standard deviation analysis','Date':str(datetime.date.today())},index=[0]),ignore_index=True)\n",
    "    columnsTitles=['Constituent','Last 12 months','Last 3 years','Table','Date']\n",
    "    #standard_dev_table = standard_dev_table.sort_values('Mean std dev(quarterly)',axis=0, ascending=True).reset_index(drop=True)\n",
    "    standard_dev_table =standard_dev_table.reindex(columns=columnsTitles)\n",
    "    #standard_dev_table.to_csv('standard_dev_table.csv', encoding = 'utf-8', index = False)\n",
    "    return standard_dev_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average True Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Calculate the 14-day Average True Range\n",
    "##For the first 14 days, TR = High-Low\n",
    "##For the days after: ATR(current) = (ATR(previous) x 13 + TR)/14\n",
    "def ATR_calculate(his):\n",
    "    TR = his['daily_high'].iloc[0:14]-his['daily_low'].iloc[0:14]\n",
    "    ATR0 = TR.mean()\n",
    "    n = his.shape[0]\n",
    "    ATR_array = np.zeros(n)\n",
    "    ATR_array[13]=ATR0\n",
    "    for i in np.arange(14,n):\n",
    "        ATR = (his['daily_high'].iloc[i] - his['daily_low'].iloc[i] + ATR0 * 13)/14.0\n",
    "        ATR_array[i] = ATR\n",
    "        ATR0 = ATR\n",
    "    return ATR_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##Record the current ATR, the average ATR of this year, the average ATR in the last 5 years\n",
    "def ATR_collection():\n",
    "    collection = db['historical']\n",
    "    ATR_table = pd.DataFrame()\n",
    "    all_constituents = ['Allianz', 'adidas', 'BASF', 'Bayer', 'Beiersdorf','BMW', 'Commerzbank', 'Continental', 'Daimler','Deutsche Bank', 'Deutsche Börse', 'Deutsche Post','Deutsche Telekom', 'EON', 'Fresenius', 'HeidelbergCement', 'Infineon','Linde','Lufthansa', 'Merck', 'RWE', 'SAP', 'Siemens', 'thyssenkrupp','Vonovia','Fresenius Medical Care','Münchener Rückversicherungs-Gesellschaft','ProSiebenSat1 Media','Volkswagen (VW) vz']\n",
    "\n",
    "    for constituent in all_constituents:\n",
    "        his = collection.find({\"constituent\":constituent})\n",
    "        his = pd.DataFrame(list(his))\n",
    "        his = his.iloc[::-1]\n",
    "        \n",
    "        ATR_array = ATR_calculate(his)\n",
    "        ATR_table = ATR_table.append(pd.DataFrame({'Constituent': constituent,'Current 14-day ATR': round(ATR_array[-1],2), 'Average ATR in the last 12 months': round(ATR_array[-252:].mean(),2), 'Average ATR in the last 3 years':round(ATR_array[-756:].mean(),2),'Table':'ATR analysis','Date':str(datetime.date.today())}, index=[0]), ignore_index=True)\n",
    "    \n",
    "    columnsTitles=['Constituent','Current 14-day ATR','Average ATR in the last 12 months', 'Average ATR in the last 3 years','Table','Date']\n",
    "    ATR_table=ATR_table.reindex(columns=columnsTitles)\n",
    "    ATR_table\n",
    "    return ATR_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Financial Performance of the Company"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Return on Capital Employed (ROCE) is a ratio that indicates the profitability and efficiency of a company, i.e. its profit vs. the total amount of capital used (see formula below).  \n",
    "\n",
    "### Return on Capital Employed = annual net profit/total assets – total liabilities\n",
    "\n",
    "### Sale is one of the biggest sources of profits for most of the equities, hence also taken into account to assess the financial ability of a company. \n",
    "\n",
    "### Dividend\n",
    "\n",
    "### Profit margin = income to the company per euro of revenue/sale. \n",
    "\n",
    "### PER - investor expectation\n",
    "\n",
    "### EPS - company's profitability from investment\n",
    "\n",
    "### EBITDA - company's earning before tax, amortization and depreciation, reflects profitability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Return on Capital Employed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ROCE_calculate(master):\n",
    "    master = master[['EBITDA in Mio','Net debt in Mio','Total assetts in Mio','year']].dropna(thresh=2)\n",
    "    net_profit = master[['EBITDA in Mio','year']].dropna(0,'any')\n",
    "    net_debt = master[['Net debt in Mio','year']].dropna(0,'any')\n",
    "    total_assets=master[['Total assetts in Mio','year']].dropna(0,'any')\n",
    "    joined = pd.merge(pd.merge(net_profit,net_debt,on='year'),total_assets,on='year')\n",
    "    joined[\"EBITDA in Mio\"] = joined[\"EBITDA in Mio\"].str.replace(\",\",\"\").astype(float)\n",
    "    joined['Net debt in Mio'] = joined['Net debt in Mio'].str.replace(\",\",\"\").astype(float)\n",
    "    joined['Total assetts in Mio'] = joined['Total assetts in Mio'].str.replace(\",\",\"\").astype(float)\n",
    "    joined['ROCE']=joined[\"EBITDA in Mio\"]*100/(joined['Total assetts in Mio']-joined['Net debt in Mio'])\n",
    "    #print joined\n",
    "    pct_ROCE_last_year = 100*(float(joined['ROCE'].loc[joined['year']==2016])-float(joined['ROCE'].loc[joined['year']==2015]))/float(joined['ROCE'].loc[joined['year']== 2015])\n",
    "    pct_ROCE_four_years = 100*(float(joined['ROCE'].loc[joined['year']==2016])-float(joined['ROCE'].loc[joined['year']==2013]))/float(joined['ROCE'].loc[joined['year']== 2013])\n",
    "    return float(pct_ROCE_last_year), float(pct_ROCE_four_years), joined[['ROCE','year']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sales Revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sales_calculate(master):\n",
    "    table= master[['Sales in Mio','year']].dropna(thresh=2)\n",
    "    #print table\n",
    "    table['Sales in Mio']=table['Sales in Mio'].str.replace(\",\",\"\").astype(float)\n",
    "    #print float(table['Sales in Mio'].iloc[-1])\n",
    "    pct_sales_last_year = 100*(float(table['Sales in Mio'].iloc[-1])-float(table['Sales in Mio'].iloc[-2]))/float(table['Sales in Mio'].iloc[-2])\n",
    "    pct_sales_four_years = 100*(float(table['Sales in Mio'].iloc[-1])-float(table['Sales in Mio'].iloc[-4]))/float(table['Sales in Mio'].iloc[-4])\n",
    "    return float(pct_sales_last_year), float(pct_sales_four_years), table['Sales in Mio']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROCE and Sales analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##Table for company performance, ROCE and Sales Revenue\n",
    "def ROCE_and_sales_collection():\n",
    "    collection = db_old['company_data']\n",
    "    ROCE_coll_table = pd.DataFrame()\n",
    "    sales_coll_table = pd.DataFrame()\n",
    "    #'Commerzbank' after 'BMW', all debt NaN,Deutsche Bank' after'Daimler',no data avaliable for 'Volkswagen (VW) vz'ranked last\n",
    "    all_constituents = ['Allianz', 'adidas', 'BASF', 'Bayer', 'Beiersdorf','BMW', 'Continental', 'Daimler', 'Deutsche Börse', 'Deutsche Post','Deutsche Telekom', 'EON', 'Fresenius', 'HeidelbergCement', 'Infineon','Linde','Lufthansa', 'Merck', 'RWE', 'SAP', 'Siemens', 'thyssenkrupp','Vonovia','Fresenius Medical Care','Münchener Rückversicherungs-Gesellschaft','ProSiebenSat1 Media']\n",
    "    for constituent in all_constituents:\n",
    "        master = collection.find({\"constituent\":constituent,'table':'Historical Key Data'})\n",
    "        master = pd.DataFrame(list(master))\n",
    "        pct_ROCE_last_year, pct_ROCE_four_years, ROCE_table = ROCE_calculate(master)\n",
    "        pct_sales_last_year, pct_sales_four_years, sales_table = sales_calculate(master)\n",
    "        \n",
    "        if (pct_ROCE_last_year>0) & (pct_ROCE_four_years>0):\n",
    "            ROCE_score = 2\n",
    "        elif pct_ROCE_last_year>0:\n",
    "            ROCE_score =1\n",
    "        else: \n",
    "            ROCE_score = 0\n",
    "        \n",
    "        if (pct_sales_last_year>0) & (pct_sales_four_years>0):\n",
    "            sales_score = 2\n",
    "        elif pct_sales_last_year>0:\n",
    "            sales_score =1\n",
    "        else: \n",
    "            sales_score = 0\n",
    "        \n",
    "        ROCE_coll_table = ROCE_coll_table.append(pd.DataFrame({'Constituent': constituent, 'Current ROCE': round(ROCE_table['ROCE'].iloc[-1],2), '% change in ROCE from previous year':round(pct_ROCE_last_year,2),'% change in ROCE from 4 years ago': round(pct_ROCE_four_years,2),'ROCE score':ROCE_score,'Table':'ROCE analysis','Date':str(datetime.date.today())}, index=[0]), ignore_index=True)\n",
    "        sales_coll_table = sales_coll_table.append(pd.DataFrame({'Constituent': constituent,'Current sales in Mio':round(sales_table.iloc[-1],2), '%change in Sales from previous year':round(pct_sales_last_year,2),'%change in Sales from 4 years ago':round(pct_sales_four_years,2),'Sales score':sales_score,'Table':'Sales analysis','Date':str(datetime.date.today())},index=[0]),ignore_index=True)\n",
    "        \n",
    "    columnsTitles_ROCE = ['Constituent', 'Current ROCE','% change in ROCE from previous year','% change in ROCE from 4 years ago','ROCE score','Table','Date']\n",
    "    columnsTitles_sales=['Constituent','Current sales in Mio', '%change in Sales from previous year','%change in Sales from 4 years ago','Sales score','Table','Date']\n",
    "    \n",
    "    ROCE_coll_table =ROCE_coll_table.reindex(columns=columnsTitles_ROCE)\n",
    "    sales_coll_table =sales_coll_table.reindex(columns=columnsTitles_sales)\n",
    "    return ROCE_coll_table, sales_coll_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividend and Dividend Yield Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Computes the linear regression model for dividend, and produce list of years where dividend is offered. \n",
    "def dividend_analysis(div):\n",
    "    div = div[['Value','Last Dividend Payment']].dropna(thresh=1)\n",
    "    div = pd.DataFrame(div)\n",
    "    value = [unicode(x) for x in div[\"Value\"]]\n",
    "    value = [x.replace(u'\\u20ac',\"\") for x in value]\n",
    "    value = [float(x) for x in value]\n",
    "    n=len(value)\n",
    "    current_div = value[0] \n",
    "    z = np.polyfit(range(n),value[::-1],1)\n",
    "    estimation = [x*z[0]+z[1] for x in range(n)]\n",
    "    res = map(operator.sub, value[::-1], estimation)\n",
    "    mse = sum([x**2 for x in res])/n*1.0\n",
    "    ##Find out the years where dividend is offered\n",
    "    #Volkswagen (VW) vz, BMW, RWE: half year\n",
    "    date_list = pd.DatetimeIndex(div['Last Dividend Payment'])\n",
    "    year_list = date_list.year\n",
    "    return z[0],z[1],mse,year_list,current_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Dividend yield is not avaliable\n",
    "def dividend_yield_analysis(master):\n",
    "    dividend_yield_table = master[['Dividend yield %','year']].dropna(thresh=2)\n",
    "    #if constituent !='Commerzbank':\n",
    "    dividend_yield_table['Dividend yield %']=dividend_yield_table['Dividend yield %'].str.replace(\"%\",\"\")\n",
    "    ##drop the empty cells and convert to float\n",
    "    filter = dividend_yield_table['Dividend yield %'] != ''\n",
    "    dividend_yield_table['Dividend yield %']=dividend_yield_table[filter].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##This dividend table stores the results of linear regression, and the list of years when dividends are offered\n",
    "##'Commerzbank','Deutsche Bank','Lufthansa','RWE','thyssenkrupp','Vonovia','Volkswagen (VW) vz'\n",
    "def dividend_collection():\n",
    "    collection = db_old['company_data']\n",
    "    collection2 = db['historical']\n",
    "    dividend_table = pd.DataFrame()\n",
    "    #all_constituents = ['Allianz', 'adidas', 'BASF', 'Bayer', 'Beiersdorf','BMW', 'Commerzbank','Continental', 'Daimler', 'Deutsche Börse', 'Deutsche Post','Deutsche Telekom', 'EON', 'Fresenius', 'HeidelbergCement', 'Infineon','Linde', 'Merck', 'SAP', 'Siemens','Fresenius Medical Care','Münchener Rückversicherungs-Gesellschaft','ProSiebenSat1 Media']\n",
    "    all_constituents = ['Allianz', 'adidas', 'BASF', 'Bayer', 'Beiersdorf','BMW', 'Commerzbank', 'Continental', 'Daimler','Deutsche Bank', 'Deutsche Börse', 'Deutsche Post','Deutsche Telekom', 'EON', 'Fresenius', 'HeidelbergCement', 'Infineon','Linde','Lufthansa', 'Merck', 'RWE', 'SAP', 'Siemens', 'thyssenkrupp','Vonovia','Fresenius Medical Care','Münchener Rückversicherungs-Gesellschaft','ProSiebenSat1 Media','Volkswagen (VW) vz']\n",
    "    for constituent in all_constituents:\n",
    "        div = collection.find({\"constituent\":constituent,'table':'Dividend'})\n",
    "        div = pd.DataFrame(list(div))\n",
    "        his = collection2.find({\"constituent\":constituent})\n",
    "        his = pd.DataFrame(list(his))\n",
    "        current_price = float(his['closing_price'].iloc[0])\n",
    "        \n",
    "        a,b,mse,year_list,current_div=dividend_analysis(div) #a represents the average growth of dividend per year in €\n",
    "        if constituent == 'BMW' or 'Volkswagen (VW) vz' or 'RWE':\n",
    "            a = a*2.0\n",
    "        \n",
    "        score =0\n",
    "        if mse<0.5:\n",
    "            score +=2\n",
    "            future_dividend = current_div + a\n",
    "            intrinsic_val = future_dividend*1.0/current_price + a/current_div\n",
    "        else: \n",
    "            future_dividend ='n/a'\n",
    "            score = score+0\n",
    "            intrinsic_val = 'n/a'\n",
    "        \n",
    "        dividend_table = dividend_table.append(pd.DataFrame({'Constituent': constituent, 'Current dividend': current_div, 'Average rate of dividend growth /year':round(a,2),'Mean square error of fitting': round(mse,2),'Years of dividend offer':'%s'%year_list,'Estimated dividend next year':future_dividend,'Current share price':current_price,'Gordon growth estimated return':intrinsic_val,'Dividend consistency score':score,'Table':'dividend analysis','Date':str(datetime.date.today())}, index=[0]), ignore_index=True)\n",
    "    \n",
    "    columnsTitles = ['Constituent', 'Current dividend','Average rate of dividend growth /year','Mean square error of fitting','Years of dividend offer','Current share price','Gordon growth estimated return','Dividend consistency score','Table','Date']\n",
    "    dividend_table =dividend_table.reindex(columns=columnsTitles)\n",
    "    dividend_table = dividend_table.sort_values('Current dividend',axis=0, ascending=False).reset_index(drop=True)\n",
    "    #dividend_table.to_csv('dividend_table.csv', encoding = 'utf-8', index = False)\n",
    "    return dividend_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividend yield estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 824,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#The dividend yield is incomplete\n",
    "collection1 = db_old['company_data']\n",
    "master=collection1.find({\"constituent\":'Commerzbank','table':'Historical Key Data'})\n",
    "master = pd.DataFrame(list(master))\n",
    "dividend_yield_table = master[['Dividend yield %','year']].dropna(thresh=2)\n",
    "dividend_yield_table['Dividend yield %']=dividend_yield_table['Dividend yield %'].str.replace(\"%\",\"\")\n",
    "dividend_yield_table['Dividend yield %']=dividend_yield_table['Dividend yield %'].replace('', np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profit Margin Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def profit_margin_calculator(master):\n",
    "    sales = master[['Sales in Mio','year']].dropna(thresh=2)\n",
    "    net_profit=master[['Net profit','year']].dropna(thresh=2)\n",
    "    sales['Sales in Mio']=sales['Sales in Mio'].str.replace(\",\",\"\").astype(float)\n",
    "    net_profit['Net profit']=net_profit['Net profit'].str.replace(\",\",\"\").astype(float)\n",
    "    profit_margin_table = net_profit.merge(sales,on='year',how='inner')\n",
    "    profit_margin_calculation = [float(net_profit['Net profit'].iloc[i])*100.0/float(sales['Sales in Mio'].iloc[i]) for i in range (sales.shape[0])]\n",
    "    return profit_margin_calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def profit_margin_collection():\n",
    "    collection = db_old['company_data']\n",
    "    profit_margin_table = pd.DataFrame()\n",
    "    #'Volkswagen (VW) vz' does not receive any data\n",
    "    all_constituents = ['Allianz', 'adidas', 'BASF', 'Bayer', 'Beiersdorf','BMW', 'Commerzbank', 'Continental', 'Daimler','Deutsche Bank', 'Deutsche Börse', 'Deutsche Post','Deutsche Telekom', 'EON', 'Fresenius', 'HeidelbergCement', 'Infineon','Linde','Lufthansa', 'Merck', 'RWE', 'SAP', 'Siemens', 'thyssenkrupp','Vonovia','Fresenius Medical Care','Münchener Rückversicherungs-Gesellschaft','ProSiebenSat1 Media']\n",
    "    for constituent in all_constituents:\n",
    "        master = collection.find({\"constituent\":constituent,'table':'Historical Key Data'})\n",
    "        master = pd.DataFrame(list(master))\n",
    "        profit_margin_calculation = profit_margin_calculator(master)\n",
    "        current_pm = round(profit_margin_calculation[-1],2)\n",
    "        pm_last_year=round(profit_margin_calculation[-2],2)\n",
    "        pm_four_years_ago=round(profit_margin_calculation[-4],2)\n",
    "        pct_last_year=(current_pm -pm_last_year)*100.0/pm_last_year\n",
    "        pct_four_years_ago=(current_pm -pm_four_years_ago)*100.0/pm_four_years_ago\n",
    "          \n",
    "        if (pct_last_year>0) & (pct_four_years_ago>0):\n",
    "            score = 2\n",
    "        elif pct_last_year>0:\n",
    "            score =1\n",
    "        else: \n",
    "            score = 0\n",
    "        \n",
    "        profit_margin_table = profit_margin_table.append(pd.DataFrame({'Constituent': constituent, 'Current profit margin':current_pm,'Profit margin last year':pm_last_year,'% change in profit margin last year':pct_last_year,'Profit margin 4 years ago': pm_four_years_ago,'% change in profit margin 4 years ago':pct_four_years_ago,'Table':'profit margin analysis','Profit margin score':score,'Date':str(datetime.date.today()) }, index=[0]), ignore_index=True)\n",
    "    columnsTitles = ['Constituent', 'Current profit margin','Profit margin last year','Profit margin 4 years ago','Profit margin score','Table','Date']\n",
    "    profit_margin_table =profit_margin_table.reindex(columns=columnsTitles)\n",
    "    profit_margin_table = profit_margin_table.sort_values('Current profit margin',axis=0, ascending=False).reset_index(drop=True)\n",
    "    #dividend_table.to_csv('dividend_table.csv', encoding = 'utf-8', index = False)\n",
    "    return profit_margin_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PER Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#'Volkswagen (VW) vz' not found\n",
    "def PER_collection():\n",
    "    n=0\n",
    "    collection = db_old['company_data']\n",
    "    PER_table = pd.DataFrame()\n",
    "    all_constituents = ['Allianz', 'adidas', 'BASF', 'Bayer', 'Beiersdorf','BMW', 'Commerzbank', 'Continental', 'Daimler','Deutsche Bank', 'Deutsche Börse', 'Deutsche Post','Deutsche Telekom', 'EON', 'Fresenius', 'HeidelbergCement', 'Infineon','Linde','Lufthansa', 'Merck', 'RWE', 'SAP', 'Siemens', 'thyssenkrupp','Vonovia','Fresenius Medical Care','Münchener Rückversicherungs-Gesellschaft','ProSiebenSat1 Media']\n",
    "    #all_constituents = ['Allianz']\n",
    "    for constituent in all_constituents:\n",
    "        master = collection.find({\"constituent\":constituent,'table':'Historical Key Data'})\n",
    "        master = pd.DataFrame(list(master))\n",
    "        PER_df = master[['PER','year']].dropna(thresh=2)\n",
    "        PER_df['PER'] = PER_df['PER'].str.replace(\",\",\"\").astype(float)\n",
    "        current_PER = float(PER_df['PER'].iloc[-1])\n",
    "        last_year_PER = float(PER_df['PER'].iloc[-2])\n",
    "        four_years_ago_PER = float(PER_df['PER'].iloc[-4])\n",
    "        pct_last_year = (current_PER-last_year_PER)*100.0/last_year_PER\n",
    "        pct_four_years = (current_PER-four_years_ago_PER)*100.0/four_years_ago_PER\n",
    "        \n",
    "        if (pct_last_year>0) & (pct_four_years>0):\n",
    "            score = 2\n",
    "        elif pct_last_year>0:\n",
    "            score =1\n",
    "        else: \n",
    "            score = 0\n",
    "        \n",
    "        PER_table = PER_table.append(pd.DataFrame({'Constituent': constituent, 'Current PER':current_PER,'PER last year':last_year_PER,'% change in PER from last year':pct_last_year,'PER 4 years ago': four_years_ago_PER,'% change in PER from 4 years ago':pct_last_year,'PER score':score,'Table':'PER analysis','Date':str(datetime.date.today())}, index=[0]), ignore_index=True)\n",
    "    columnsTitles = ['Constituent', 'Current PER','PER last year','PER 4 years ago','PER score','Table','Date']\n",
    "    PER_table =PER_table.reindex(columns=columnsTitles)\n",
    "    PER_table = PER_table.sort_values('Current PER',axis=0, ascending=False).reset_index(drop=True)\n",
    "    #dividend_table.to_csv('dividend_table.csv', encoding = 'utf-8', index = False)\n",
    "    return PER_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EPS Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#No data avaliable for'Volkswagen (VW) vz'\n",
    "def EPS_collection():\n",
    "    collection = db_old['company_data']\n",
    "    EPS_table = pd.DataFrame()\n",
    "    all_constituents = ['Allianz', 'adidas', 'BASF', 'Bayer', 'Beiersdorf','BMW', 'Commerzbank', 'Continental', 'Daimler','Deutsche Bank', 'Deutsche Börse', 'Deutsche Post','Deutsche Telekom', 'EON', 'Fresenius', 'HeidelbergCement', 'Infineon','Linde','Lufthansa', 'Merck', 'RWE', 'SAP', 'Siemens', 'thyssenkrupp','Vonovia','Fresenius Medical Care','Münchener Rückversicherungs-Gesellschaft','ProSiebenSat1 Media']\n",
    "    #all_constituents = ['Allianz', 'adidas', 'BASF', 'Bayer', 'Beiersdorf','BMW', 'Commerzbank', 'Continental', 'Daimler','Deutsche Bank', 'Deutsche Börse', 'Deutsche Post','Deutsche Telekom', 'EON', 'Fresenius', 'HeidelbergCement', 'Infineon','Linde','Lufthansa', 'Merck', 'RWE', 'SAP', 'Siemens', 'thyssenkrupp','Vonovia','Fresenius Medical Care','Münchener Rückversicherungs-Gesellschaft','ProSiebenSat1 Media','Volkswagen (VW) vz']\n",
    "    for constituent in all_constituents:\n",
    "        #print constituent\n",
    "        master = collection.find({\"constituent\":constituent,'table':'Historical Key Data'})\n",
    "        master = pd.DataFrame(list(master))\n",
    "        EPS = master[['Earning per share','year']].dropna(thresh=2)\n",
    "        current_EPS = float(EPS['Earning per share'].iloc[-1])\n",
    "        last_year_EPS = float(EPS['Earning per share'].iloc[-2])\n",
    "        four_years_ago_EPS = float(EPS['Earning per share'].iloc[-4])\n",
    "        pct_last_year = (current_EPS-last_year_EPS)*100.0/last_year_EPS\n",
    "        pct_four_years = (current_EPS-four_years_ago_EPS)*100.0/four_years_ago_EPS\n",
    "        if (pct_last_year>0) & (pct_four_years>0):\n",
    "            score = 2\n",
    "        elif pct_last_year>0:\n",
    "            score =1\n",
    "        else: \n",
    "            score = 0\n",
    "        EPS_table = EPS_table.append(pd.DataFrame({'Constituent': constituent, 'Current EPS':current_EPS,'EPS last year':last_year_EPS, '% change in EPS from last year': round(pct_last_year,2),'EPS score': score,'EPS 4 years ago': four_years_ago_EPS,'% change in EPS from 4 years ago':round(pct_four_years,2),'Table':'EPS analysis','Date':str(datetime.date.today()) }, index=[0]), ignore_index=True)\n",
    "    columnsTitles = ['Constituent', 'Current EPS','EPS last year','% change in EPS from last year','EPS 4 years ago','% change in EPS from 4 years ago','EPS score','Table','Date']\n",
    "    EPS_table =EPS_table.reindex(columns=columnsTitles)\n",
    "    EPS_table = EPS_table.sort_values('Current EPS',axis=0, ascending=False).reset_index(drop=True)\n",
    "    return EPS_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EBITDA Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def EBITDA_collection():\n",
    "    collection = db_old['company_data']\n",
    "    EBITDA_table = pd.DataFrame()\n",
    "    #No data avaliable for'Volkswagen (VW) vz', Commerzbank, Deutsche Bank\n",
    "    all_constituents = ['Allianz', 'adidas', 'BASF', 'Bayer', 'Beiersdorf','BMW', 'Continental', 'Daimler', 'Deutsche Börse', 'Deutsche Post','Deutsche Telekom', 'EON', 'Fresenius', 'HeidelbergCement', 'Infineon','Linde','Lufthansa', 'Merck', 'RWE', 'SAP', 'Siemens', 'thyssenkrupp','Vonovia','Fresenius Medical Care','Münchener Rückversicherungs-Gesellschaft','ProSiebenSat1 Media']\n",
    "    #all_constituents = ['Allianz', 'adidas', 'BASF', 'Bayer', 'Beiersdorf','BMW', 'Commerzbank', 'Continental', 'Daimler','Deutsche Bank', 'Deutsche Börse', 'Deutsche Post','Deutsche Telekom', 'EON', 'Fresenius', 'HeidelbergCement', 'Infineon','Linde','Lufthansa', 'Merck', 'RWE', 'SAP', 'Siemens', 'thyssenkrupp','Vonovia','Fresenius Medical Care','Münchener Rückversicherungs-Gesellschaft','ProSiebenSat1 Media','Volkswagen (VW) vz']\n",
    "    for constituent in all_constituents:\n",
    "        master = collection.find({\"constituent\":constituent,'table':'Historical Key Data'})\n",
    "        master = pd.DataFrame(list(master))\n",
    "        EBITDA = master[['EBITDA in Mio','year']].dropna(thresh=2)\n",
    "        EBITDA[\"EBITDA in Mio\"] = EBITDA[\"EBITDA in Mio\"].str.replace(\",\",\"\").astype(float)\n",
    "        current_EBITDA = float(EBITDA['EBITDA in Mio'].iloc[-1])\n",
    "        last_year_EBITDA = float(EBITDA['EBITDA in Mio'].iloc[-2])\n",
    "        four_years_ago_EBITDA = float(EBITDA['EBITDA in Mio'].iloc[-4])\n",
    "        pct_last_year = (current_EBITDA-last_year_EBITDA)*100.0/last_year_EBITDA\n",
    "        pct_four_years = (current_EBITDA-four_years_ago_EBITDA)*100.0/four_years_ago_EBITDA\n",
    "        if (pct_last_year>0) & (pct_four_years>0):\n",
    "            score = 2\n",
    "        elif pct_last_year>0:\n",
    "            score =1\n",
    "        else: \n",
    "            score = 0\n",
    "        EBITDA_table = EBITDA_table.append(pd.DataFrame({'Constituent': constituent, 'Current EBITDA in Mio':current_EBITDA,'EBITDA last year in Mio':last_year_EBITDA, '% change in EBITDA from last year': round(pct_last_year,2),'EBITDA score': score,'EBITDA 4 years ago in Mio': four_years_ago_EBITDA,'% change in EBITDA from 4 years ago':round(pct_four_years,2),'Table':'EBITDA analysis','Date':str(datetime.date.today()) }, index=[0]), ignore_index=True)\n",
    "    columnsTitles = ['Constituent', 'Current EBITDA in Mio','EBITDA last year in Mio','% change in EBITDA from last year','EBITDA 4 years ago in Mio','% change in EBITDA from 4 years ago','EBITDA score','Table','Date']\n",
    "    EBITDA_table =EBITDA_table.reindex(columns=columnsTitles)\n",
    "    EBITDA_table = EBITDA_table.sort_values('Current EBITDA in Mio',axis=0, ascending=False).reset_index(drop=True)\n",
    "    return EBITDA_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Market Signal "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Momentum - RSI Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RSI_calculate(his,n):\n",
    "    delta = his['closing_price'].diff()\n",
    "    dUp, dDown = delta.copy(), delta.copy()\n",
    "    dUp[dUp < 0] = 0\n",
    "    dDown[dDown > 0] = 0    \n",
    "    RolUp = dUp.rolling(window=n).mean()\n",
    "    RolDown = dDown.rolling(window=n).mean().abs()\n",
    "    RS = RolUp/RolDown+0.0\n",
    "    a=RS.shape[0]\n",
    "    RSI = np.zeros(a)\n",
    "    for i in np.arange(n,a):\n",
    "        RSI[i] = 100-100/(1.0+RS[i])\n",
    "    #If > 70: overbought signal, <30: oversold signal\n",
    "    RSI_last_year = RSI[-252:]\n",
    "    #print RSI[-90:]\n",
    "    overbought = (RSI_last_year>=70)\n",
    "    oversold = (RSI_last_year<=30)\n",
    "    overbought_count = RSI_last_year[overbought].shape[0]\n",
    "    oversold_count = RSI_last_year[oversold].shape[0]\n",
    "    return RSI[-1],overbought_count/252.0,oversold_count/252.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Golden Cross Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##This (very long) function returns the following quantities: \n",
    "#Crossing frequency - On average the number of days it takes for a Golden Cross/Death Cross to happen\n",
    "#Average duration of Golden Cross period - time elapsed from the occurrence of Golden Cross till the occurrence of Death Cross\n",
    "#Average golden growth - the average % growth in share price during a Golden Cross period. \n",
    "##The function takes a dataframe of the historical company data as an input. \n",
    "\n",
    "def crossing_analysis(his):\n",
    "    his['SMA-50'] = his['closing_price'].rolling(window=50,center=False).mean()\n",
    "    his['SMA-200']= his['closing_price'].rolling(window=200,center=False).mean()\n",
    "    previous_50 = his['SMA-50'].shift(1)\n",
    "    previous_200 = his['SMA-200'].shift(1)\n",
    "\n",
    "#Identify all the crosses, calculate the number of days on average a cross occurs. \n",
    "    crosses = (((his['SMA-50'] <= his['SMA-200']) & (previous_50 >= previous_200))|((his['SMA-50']>= his['SMA-200']) & (previous_50 <= previous_200)))\n",
    "    crossing_dates = his.loc[crosses, 'date']\n",
    "    crossing_index = crosses[crosses==True].index.tolist()\n",
    "    freq = crossing_dates.diff().mean().days\n",
    "\n",
    "##Idenfity death and golden crosses \n",
    "    death_crosses = ((his['SMA-50'] <= his['SMA-200']) & (previous_50 >= previous_200))\n",
    "    golden_crosses = ((his['SMA-50']>= his['SMA-200']) & (previous_50 <= previous_200))\n",
    "\n",
    "##calculating the average duration of golden cross\n",
    "    golden_crossing_dates = his.loc[golden_crosses, 'date']\n",
    "    death_crossing_dates = his.loc[death_crosses,'date']\n",
    "\n",
    "    golden_crossing_dates = pd.DataFrame(golden_crossing_dates)\n",
    "    death_crossing_dates = pd.DataFrame(death_crossing_dates)\n",
    "\n",
    "    golden_crossing_index = golden_crosses[golden_crosses==True].index.tolist()\n",
    "    death_crossing_index = death_crosses[death_crosses==True].index.tolist() \n",
    "\n",
    "##Find the average golden cross duration\n",
    "    durations =0 #initialize\n",
    "    growth_sum = 0\n",
    "    num_crossings = min(golden_crossing_dates.shape[0],death_crossing_dates.shape[0])\n",
    "    \n",
    "#Different calculations depending on which of golden and death crosses occur the earliest\n",
    "    if golden_crossing_dates['date'].iloc[0]<death_crossing_dates['date'].iloc[0]:\n",
    "        for i in range(num_crossings):\n",
    "            tf = death_crossing_dates['date'].iloc[i]-golden_crossing_dates['date'].iloc[i]\n",
    "            durations = durations+int(tf.days)\n",
    "        #Take either SMA-50 or SMA-200. At the point of crossing, the SMAs are identical and are already very close to the actual price in that instant\n",
    "            golden_price = his['SMA-50'].loc[his['date'] == golden_crossing_dates['date'].iloc[i]]\n",
    "            death_price = his['SMA-50'].loc[his['date'] == death_crossing_dates['date'].iloc[i]]\n",
    "        #calculate rowth as a proportion to the price when golden cross occurs. \n",
    "            growth = (float(death_price)-float(golden_price))/(0.0+float(golden_price))\n",
    "            growth_sum = growth_sum+growth\n",
    "        \n",
    "    else:\n",
    "        for i in np.arange(1,num_crossings):\n",
    "            tf = death_crossing_dates['date'].iloc[i]-golden_crossing_dates['date'].iloc[i-1]\n",
    "            durations = durations+int(tf.days)\n",
    "            golden_price = his['SMA-50'].loc[his['date'] == golden_crossing_dates['date'].iloc[i-1]]\n",
    "            death_price = his['SMA-50'].loc[his['date'] == death_crossing_dates['date'].iloc[i]]\n",
    "            growth = (float(death_price)-float(golden_price))/(0.0+float(golden_price))\n",
    "            growth_sum = growth_sum+growth\n",
    "\n",
    "            \n",
    "    #Obtain the average golden cross duration in terms of days.        \n",
    "    ave_golden_duration = durations*1.0/golden_crossing_dates.shape[0]\n",
    "    ave_death_duration = (his.shape[0] - durations*1.0)/death_crossing_dates.shape[0]\n",
    "\n",
    "    ##Obtain the average growth in proportion of share price during golden crossing\n",
    "    ave_golden_growth = growth_sum/(1.0*golden_crossing_dates.shape[0])\n",
    "    \n",
    "    ####Obtain the current crossing state of stock\n",
    "    if golden_crossing_dates['date'].iloc[-1]> death_crossing_dates['date'].iloc[-1]:\n",
    "        recent_cross = 'Golden Cross'\n",
    "        #Measures how much is the short-term moving average is above the long-term moving average after a Golden Cross\n",
    "        difference = his['SMA-50'].iloc[golden_crossing_index[-1]:]- his['SMA-200'].iloc[golden_crossing_index[-1]:] \n",
    "        #This records how the difference between SMA-50 and SMA-200 varies over time. (Are they still diverging from each other?)\n",
    "        n = difference.shape[0]\n",
    "        difference_diff = difference.diff()\n",
    "    \n",
    "        ##Only focus on the most recent 1/3 of the price after Golden Cross. \n",
    "        ##Proportional to the average gradient of the difference between SMA-50 AND SMA-200, assuming that the time interval of data collection is consistent. \n",
    "        difference_coefficient = difference_diff[int(n*2/3):n].mean()\n",
    "        ## DO SOMETHING WITH THIS !\n",
    "    \n",
    "        if difference_coefficient > 0: \n",
    "            diverge = \"continues diverging (Bull)\"\n",
    "        else:\n",
    "            diverge = \"starts converging to SMA 200\"\n",
    "        \n",
    "         ##Find the peak after the gold crossing \n",
    "        his_after_gold_cross =his.iloc[golden_crossing_index[-1]:] #truncate the historical price data\n",
    "        his_max_price = his.ix[his_after_gold_cross['closing_price'].idxmax()] #extract the row in historical price data when price is maximum after crossing\n",
    "        max_price = round(his_max_price['closing_price'],2) \n",
    "        date_max_price = his_max_price['date'] \n",
    "        \n",
    "        ##Find the minimum price recorded after the gold crossing \n",
    "        his_min_price = his.ix[his_after_gold_cross['closing_price'].idxmin()] #extract the row in historical price data when price is maximum after crossing\n",
    "        min_price = round(his_min_price['closing_price'],2)\n",
    "        date_min_price = his_min_price['date'] \n",
    "        \n",
    "        #Find how many days elapsed from the day when the peak price till the day of latest price collection\n",
    "        days_after_extreme = int((his['date'].iloc[-1] - date_max_price).days)\n",
    "        pct_diff_from_extreme = round((his['closing_price'].iloc[-1]-his_max_price['closing_price'])*100.0/his_max_price['closing_price'],2)\n",
    "        if days_after_extreme == 0: \n",
    "            extreme_status = 'Still increasing/reaching a maximum'\n",
    "        else:\n",
    "            extreme_status = '%s days'%days_after_extreme +' off the maximum at %s'%max_price\n",
    "       \n",
    "        \n",
    "    else:\n",
    "        recent_cross= 'Death Cross'\n",
    "        difference = his['SMA-200'].iloc[golden_crossing_index[-1]:] -his['SMA-50'].iloc[golden_crossing_index[-1]:]\n",
    "    #This records how the difference between SMA-50 and SMA-200 varies over time. (Are they still diverging from each other?)\n",
    "        n = difference.shape[0]\n",
    "        difference_diff = difference.diff()\n",
    "    \n",
    "    ##Only focus on the most recent 1/3 of the price after Death Cross. \n",
    "    ##Proportional to the average gradient of the difference between SMA-50 AND SMA-200, assuming that the time interval of data collection is consistent. \n",
    "        difference_coefficient = difference_diff[int(n*2/3):n].mean()\n",
    "    \n",
    "    ## INSTEAD OF 2/3, USE PEAK VALUE POSITIONS!!\n",
    "    \n",
    "        if difference_coefficient > 0: \n",
    "            diverge = \"continues diverging (Bear)\"\n",
    "        else:\n",
    "            diverge = \"starts converging to SMA 200\"\n",
    "        \n",
    "         ##Find the trough after the death crossing \n",
    "        his_after_death_cross =his.iloc[death_crossing_index[-1]:] #truncate the historical price data\n",
    "        his_min_price = his.ix[his_after_death_cross['closing_price'].idxmin()] #extract the row in historical price data when price is maximum after crossing\n",
    "        min_price = round(his_min_price['closing_price'],2)\n",
    "        date_min_price = his_min_price['date'] \n",
    "        \n",
    "        ##Find the maximum price after the death crossing \n",
    "        his_max_price = his.ix[his_after_death_cross['closing_price'].idxmax()] #extract the row in historical price data when price is maximum after crossing\n",
    "        max_price = round(his_max_price['closing_price'],2)\n",
    "        date_max_price = his_max_price['date'] \n",
    "        \n",
    "        \n",
    "        #Find how many days elapsed from the day when the peak price till the day of latest price collection\n",
    "        days_after_extreme = int((his['date'].iloc[-1] - date_min_price).days)\n",
    "        pct_diff_from_extreme = round((his['closing_price'].iloc[-1]-his_min_price['closing_price'])*100.0/his_max_price['closing_price'],2)\n",
    "        if days_after_extreme: \n",
    "            extreme_status = 'Still decreasing/reaching a minimum'\n",
    "        else:\n",
    "            extreme_status = '%s days'%days_after_extreme +' off the minimum at %s'%min_price\n",
    "    \n",
    "    \n",
    "    return freq,ave_golden_duration,ave_golden_growth,recent_cross,diverge,max_price,min_price,days_after_extreme,extreme_status,pct_diff_from_extreme\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Note: Henkel_vs(Henkel vs) does not have data\n",
    "def crossing_and_RSI_collection():\n",
    "    collection = db['historical']\n",
    "    table = pd.DataFrame()\n",
    "    all_constituents = ['Allianz', 'adidas', 'BASF', 'Bayer', 'Beiersdorf','BMW', 'Commerzbank', 'Continental', 'Daimler','Deutsche Bank', 'Deutsche Börse', 'Deutsche Post','Deutsche Telekom', 'EON', 'Fresenius', 'HeidelbergCement', 'Infineon','Linde','Lufthansa', 'Merck', 'RWE', 'SAP', 'Siemens', 'thyssenkrupp','Vonovia','Fresenius Medical Care','Münchener Rückversicherungs-Gesellschaft','ProSiebenSat1 Media','Volkswagen (VW) vz']\n",
    "    for constituent in all_constituents:\n",
    "        his = collection.find({\"constituent\":constituent})\n",
    "        his = pd.DataFrame(list(his))\n",
    "        his=his.iloc[::-1].reset_index(drop=True)\n",
    "        \n",
    "        \n",
    "        cross_interval,ave_golden_duration,ave_golden_growth,recent_cross,diverge,max_price,min_price,days_after_extreme,extreme_status,pct_diff_from_extreme = crossing_analysis(his)\n",
    "        RSI_current,overbought,oversold = RSI_calculate(his,14)\n",
    "        \n",
    "        #Cross_score scores the bullish trend of SMA-50 and SMA-20. \n",
    "        if (recent_cross == 'Golden Cross') & (diverge ==\"continues diverging (Bull)\"):\n",
    "            cross_score = 2\n",
    "        elif (recent_cross == 'Death Cross') & (diverge ==\"continues diverging (Bear)\"):\n",
    "            cross_score = -2\n",
    "        else:\n",
    "            cross_score = 0\n",
    "        \n",
    "        #This is a stronger measurement of bull/bear trends\n",
    "        if extreme_status == 'Still increasing/reaching a maximum':\n",
    "            cross_score +=3\n",
    "        elif extreme_status == 'Still decreasing/reaching a minimum':\n",
    "             cross_score -=3\n",
    "        else:\n",
    "            cross_score = cross_score\n",
    "        \n",
    "        #If Golden-cross period generally last longer than death, add one. \n",
    "        if ave_golden_duration>cross_interval:\n",
    "            cross_score +=1\n",
    "        else: \n",
    "            cross_score=cross_score\n",
    "            \n",
    "    \n",
    "        \n",
    "        if RSI_current > 70:\n",
    "            RSI_score = 1\n",
    "        elif RSI_current < 30:\n",
    "            RSI_score = -1\n",
    "        else: \n",
    "            RSI_score =0\n",
    "        \n",
    "        if overbought>oversold:\n",
    "            RSI_score = RSI_score+1\n",
    "        else: \n",
    "            RSI_score=RSI_score\n",
    "        \n",
    "        \n",
    "        table = table.append(pd.DataFrame({'Constituent': constituent,'Price on the latest date': float(his.iloc[-1].closing_price),'Average crossing interval (days)': cross_interval,'Crossing frequency per year':round(365.0/cross_interval,2), 'Duration of Golden Cross(days)':int(ave_golden_duration), 'Average return per Golden Cross period':round(ave_golden_growth,2),'Recent cross':recent_cross, 'Status of SMA 50':diverge,'Maximum price after cross':max_price,'Minimum price after cross':min_price,'Days elaspsed from the last max/min':days_after_extreme,'Max/min observation':extreme_status,'% change in price after max/min':pct_diff_from_extreme,'Current RSI':round(RSI_current,2),'% days overbought':round(overbought*100,2),'% days oversold':round(oversold*100,2),'Bull score (crossing)': cross_score,'Bull score (RSI)':RSI_score,'Table':'Market signal','Date':str(datetime.date.today())}, index=[0]), ignore_index=True)\n",
    "        ##Find golden cross duration times, and expected growth of stock per unit time. \n",
    "    columnsTitles=[\"Constituent\",\"Price on the latest date\",'Average crossing interval (days)','Crossing frequency per year','Duration of Golden Cross(days)','Average return per Golden Cross period','Recent cross','Status of SMA 50','Maximum price after cross','Minimum price after cross','Days elaspsed from the last max/min','Max/min observation','% change in price after max/min','Current RSI','% days overbought','% days oversold', 'Bull score (crossing)','Bull score (RSI)','Table','Date']\n",
    "    table=table.reindex(columns=columnsTitles)\n",
    "    #table.to_csv('general_info%s.csv'%datetime.date.today(), encoding = 'utf-8', index = False)\n",
    "    return table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyst Opinions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "constituents_selected=['Adidas', 'Commerzbank', 'BMW', 'Deutsche_Bank', 'EON']\n",
    "all_constituents_dict = {'Allianz':'Allianz', 'adidas':'adidas', 'BASF':'BASF', 'Bayer':'Bayer', 'Beiersdorf':'Beiersdorf',\n",
    "                    'BMW':'BMW', 'Commerzbank':'Commerzbank', 'Continental':'Continental', 'Daimler':'Daimler',\n",
    "                    'Deutsche Bank':'Deutsche_Bank', 'Deutsche Börse':'Deutsche_Boerse', 'Deutsche Post':'Deutsche_Post',\n",
    "                    'Deutsche Telekom':'Deutsche_Telekom', 'EON':'EON', 'Fresenius Medical Care':'Fresenius_Medical_Care',\n",
    "                    'Fresenius':'Fresenius', 'HeidelbergCement':'HeidelbergCement', 'Infineon':'Infineon',\n",
    "                    'Linde':'Linde','Lufthansa':'Lufthansa', 'Merck':'Merck', 'Münchener Rückversicherungs-Gesellschaft': 'Munich_Re',\n",
    "                    'ProSiebenSat1 Media':'ProSiebenSat1_Media', 'RWE':'RWE', 'Siemens':'Siemens', 'thyssenkrupp':'thyssenkrupp',\n",
    "                    'Volkswagen (VW) vz':'Volkswagen_vz','Vonovia':'Vonovia'}\n",
    "#Data for SAP is missing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Write a function that extract analyst data for all stocks\n",
    "def analyst_businessinsider(constituents_dict): \n",
    "    analyst_opinion_table = pd.DataFrame()\n",
    "    for constituent in constituents_dict:\n",
    "        \n",
    "        if constituent == 'SAP':\n",
    "            url = 'http://www.reuters.com/finance/stocks/analyst/SAP'\n",
    "            r = urllib.urlopen(url).read()\n",
    "            soup = BeautifulSoup(r,'lxml')\n",
    "            tables = soup.find_all('div', class_='moduleBody')\n",
    "            analyst_recommendation = tables[2].find_all('td',class_='data')\n",
    "            rating = float(analyst_recommendation[-4].text)\n",
    "            \n",
    "        #print constituent\n",
    "        url = 'http://markets.businessinsider.com/analyst/'+constituents_dict[constituent]\n",
    "        #url = 'http://markets.businessinsider.com/stock/'+constituents_dict[constituent]+'/analysts-opinions'\n",
    "        r = urllib.urlopen(url).read()\n",
    "        soup = BeautifulSoup(r,'lxml')\n",
    "        rating_extract = soup.find_all(\"div\",class_=\"rating\")\n",
    "        rating = float(rating_extract[0].text)\n",
    "                             \n",
    "        #Obtain the data inside the table as a resultset (list) and extract the text. \n",
    "        opinions = soup.find_all(\"td\",class_=[\"bar buy\",'bar overweight',\"bar hold\",\"bar underweight\",\"bar sell\"])\n",
    "        opinions_data = [int(x.text) for x in opinions]\n",
    "        buy_count=opinions_data[0]+opinions_data[1]\n",
    "        hold_count=opinions_data[2]\n",
    "        sell_count=opinions_data[3]+opinions_data[4]\n",
    "        total = buy_count+hold_count+ sell_count\n",
    "    \n",
    "        #Find analyst target for stock prices\n",
    "        letters = soup.find_all(\"table\",class_='table table-small no-margin-bottom')\n",
    "        letters2 = letters[0].find_all(\"td\")\n",
    "        target_list = [str(x.text.strip()) for x in letters2]\n",
    "\n",
    "        #Extract the prices. \n",
    "        median_target = round(float(target_list[5].replace(\"EUR\",\"\")),2)\n",
    "        highest_target = round(float(target_list[7].replace(\"EUR\",\"\")),2)\n",
    "        lowest_target = round(float(target_list[9].replace(\"EUR\",\"\")),2)\n",
    "\n",
    "                             \n",
    "        #Allocate a status according to the rating\n",
    "        if rating <= 2:\n",
    "            rating_result='Strong buy'\n",
    "        elif rating <= 2.8:\n",
    "            rating_result = 'Moderate buy'\n",
    "        elif rating <= 3.2:\n",
    "            rating_result = 'Hold'\n",
    "        elif rating <=4:\n",
    "            rating_result = 'Moderate sell'\n",
    "        else: \n",
    "            rating_result = 'Strong sell'\n",
    "        \n",
    "        analyst_opinion_table = analyst_opinion_table.append(pd.DataFrame({'Constituent':constituent,'Analyst rating': rating, 'Analyst recommendation': rating_result,'Buy':buy_count,'Hold':hold_count,'Sell':sell_count,'% Buy':round(buy_count*1.0/total,3),'% Hold':round(hold_count*1.0/total,3),'% Sell':round(sell_count*1.0/total,3),'Median target price':median_target, 'Highest target price':highest_target,'Lowest target price':lowest_target,'Date':datetime.date.today(),'Table':'Analyst opinions'},index=[0],),ignore_index=True)\n",
    "    columnsTitles = ['Constituent','Analyst rating','Analyst recommendation', 'Buy','Hold','Sell','% Buy','% Hold','% Sell','Median target price','Highest target price','Lowest target price','Table','Date']\n",
    "    analyst_opinion_table =analyst_opinion_table.reindex(columns=columnsTitles)\n",
    "    return analyst_opinion_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorising companies by industry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def industry_categorisation():\n",
    "    category_table = pd.DataFrame()\n",
    "    array = ['adidas','Clothing','Allianz','Insurance','BASF','Chemicals','Bayer','Pharmaceuticals','Beiersdorf','Chemicals',\n",
    " 'BMW','Manufacturing','Commerzbank','Banking','Continental','Manufacturing','Daimler','Manufacturing','Deutsche Bank','Banking',\n",
    " 'Deutsche Börse','Securities','Lufthansa','Transport Aviation','Deutsche Post','Logistics','Deutsche Telekom','Communications',\n",
    " 'EON','Energy','Fresenius','Medical','Fresenius Medical Care','Medical','HeidelbergCement','Building',\n",
    " 'Infineon','Semiconductors','Linde','Industrial gases','Merck','Pharmaceuticals','Münchener Rückversicherungs-Gesellschaft','Insurance',\n",
    " 'ProSiebenSat1 Media','Media','RWE','Energy','SAP','Software','Siemens','Industrial','thyssenkrupp','Manufacturing',\n",
    "'Volkswagen (VW) vz','Manufacturing','Vonovia','Real estate']\n",
    "    n=len(array)\n",
    "    for i in range(int(n/2)):\n",
    "        category_table = category_table.append(pd.DataFrame({'Constituent': array[i*2], 'Industry':array[2*i+1],'Table': 'category analysis' },index=[0]), ignore_index=True)\n",
    "    category_table = pd.DataFrame(category_table)\n",
    "    #category_table.to_csv('industry_category_table.csv', encoding = 'utf-8', index = False)\n",
    "    return category_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running all the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def all_analysis(all_constituents_dict):\n",
    "    category_table=industry_categorisation()\n",
    "#Profitability(baesd on historical stock price) \n",
    "    cumulative_returns_table = cumulative_returns_collection() #6mn, 1yr,3yr\n",
    "    quarter_mean_table = quarter_mean_collection()#1yr,3yr,7yr\n",
    "    market_signal_table = crossing_and_RSI_collection()\n",
    "\n",
    "#Volatility measure (based on historical stock price)\n",
    "    standard_dev_table = standard_dev_collection()#1yr,3yr\n",
    "    ATR_table = ATR_collection()#1yr,3yr\n",
    "    \n",
    "#Balance sheet analysis, may be qualitative (data only released once per year, limitations of data)\n",
    "    ROCE_table, sales_table = ROCE_and_sales_collection()\n",
    "    dividend_table = dividend_collection()\n",
    "    profit_margin_table = profit_margin_collection()\n",
    "    PER_table=PER_collection()\n",
    "    EPS_table=EPS_collection()\n",
    "    EBITDA_table = EBITDA_collection()\n",
    "    \n",
    "##Append missing values into the ROCE table\n",
    "    ROCE_table = ROCE_table.append(pd.DataFrame({'Constituent':'Commerzbank','ROCE in 2016':0.05},index=[0]),ignore_index=True)\n",
    "    ROCE_table = ROCE_table.append(pd.DataFrame({'Constituent':'Deutsche Bank','ROCE in 2016':-0.01},index=[0]),ignore_index=True)\n",
    "    ROCE_table = ROCE_table.append(pd.DataFrame({'Constituent':'Volkswagen (VW) vz','ROCE in 2016':0.03},index=[0]),ignore_index=True)\n",
    "\n",
    "##Append missing values for Volkswagen for PER and Profit Margin table\n",
    "    PER_table=PER_table.append(pd.DataFrame({'Constituent':'Volkswagen (VW) vz','Current PER':13.0},index=[0]),ignore_index=True)\n",
    "    profit_margin_table = profit_margin_table.append(pd.DataFrame({'Constituent':'Volkswagen (VW) vz','Current profit margin':0.0373,'Profit margin last year':'NaN','Profit margin 4 years ago':'NaN'},index=[0]),ignore_index=True)\n",
    "\n",
    "##Tables like sales can be used for industry comparison. \n",
    "    return category_table,cumulative_returns_table,quarter_mean_table,standard_dev_table,ATR_table,ROCE_table, sales_table ,dividend_table,profit_margin_table,PER_table,EPS_table,EBITDA_table, market_signal_table "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtaining all the result tables and uploading on MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Obtaining results table for most of fundamental analysis (uploaded on MongoDB individually)\n",
    "#Cumulative returns, quarter mean growth, ATR, dividend, Return on Capital Employed, Profit margin, PER and Sales\n",
    "industry_category_table, cumulative_returns_table,quarter_mean_table,standard_dev_table,ATR_table,ROCE_table, sales_table ,dividend_table,profit_margin_table,PER_table,EPS_table,EBITDA_table, market_signal_table =all_analysis(all_constituents_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting results to JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "## Converting all the tables into JSON for inserting into MongoDB\n",
    "##Stock price analysis, to be updated every week\n",
    "cumulative_returns_json = json.loads(cumulative_returns_table.to_json(orient='records'))\n",
    "quarter_mean_json = json.loads(quarter_mean_table.to_json(orient='records'))\n",
    "standard_dev_json = json.loads(standard_dev_table.to_json(orient='records'))\n",
    "ATR_json = json.loads(ATR_table.to_json(orient='records'))\n",
    "\n",
    "##Market json analysis\n",
    "market_signal_json = json.loads(market_signal_table.to_json(orient='records'))\n",
    "industry_category_json = json.loads(industry_category_table.to_json(orient='records'))\n",
    "\n",
    "##Balance sheet analysis, to be updated every 6 months\n",
    "ROCE_json = json.loads(ROCE_table.to_json(orient='records'))\n",
    "sales_json = json.loads(sales_table.to_json(orient='records'))\n",
    "dividend_json = json.loads(dividend_table.to_json(orient='records'))\n",
    "profit_margine_json = json.loads(profit_margin_table.to_json(orient='records'))\n",
    "PER_json = json.loads(PER_table.to_json(orient='records'))\n",
    "EPS_json = json.loads(EPS_table.to_json(orient='records'))\n",
    "EBITDA_json = json.loads(EBITDA_table.to_json(orient='records'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inserting JSON results onto MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "client_new = MongoClient('mongodb://igenie_readwrite:igenie@35.189.101.142:27017/dax_gcp')\n",
    "db = client_new.dax_gcp\n",
    "\n",
    "##Empty the result database before updating it with new results. \n",
    "db['fundamental analysis'].drop()\n",
    "db['price analysis'].drop()\n",
    "#db['ranking'].drop()\n",
    "\n",
    "collection1 = db['fundamental analysis']\n",
    "collection2 = db['price analysis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertManyResult at 0x11c5b92d0>"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Insert new results into databases. \n",
    "## insert into db['fundamental analysis']\n",
    "collection1.insert_many(industry_category_json)\n",
    "collection1.insert_many(ROCE_json) #table: ROCE analysis\n",
    "collection1.insert_many(sales_json) #table: Sales analysis\n",
    "collection1.insert_many(dividend_json) #table:dividend analysis\n",
    "collection1.insert_many(profit_margine_json) #table:profit margin analysis\n",
    "collection1.insert_many(PER_json) #table:PER analysis\n",
    "collection1.insert_many(EPS_json) #table:EPS analysis\n",
    "collection1.insert_many(EBITDA_json) #table:EBIDTA analysis\n",
    "\n",
    "\n",
    "## insert into db['price analysis']\n",
    "collection2.insert_many(cumulative_returns_json) #table: cumulative return analysis\n",
    "collection2.insert_many(quarter_mean_json) #table: quarterly growth analysis\n",
    "collection2.insert_many(standard_dev_json) #table: standard deviation analysis\n",
    "collection2.insert_many(ATR_json) #table: ATR analysis\n",
    "collection2.insert_many(market_signal_json) #table: Market signal\n",
    "collection2.insert_many(dividend_json) #table:dividend analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertManyResult at 0x1189c0e10>"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db['price analysis'].drop()\n",
    "collection2 = db['price analysis']\n",
    "collection2.insert_many(cumulative_returns_json) #table: cumulative return analysis\n",
    "collection2.insert_many(quarter_mean_json) #table: quarterly growth analysis\n",
    "collection2.insert_many(standard_dev_json) #table: standard deviation analysis\n",
    "collection2.insert_many(ATR_json) #table: ATR analysis\n",
    "collection2.insert_many(market_signal_json) #table: Market signal\n",
    "collection2.insert_many(dividend_json) #table:dividend analysis"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
